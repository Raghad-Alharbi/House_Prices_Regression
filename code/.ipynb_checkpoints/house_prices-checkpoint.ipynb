{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n\n# Project 2: Kaggle Challenges House Prices (Regression)\n\n\n\n### Group 5 team members:\n\n- Raghad Alharbi\n- Fatimah Aljohani\n- Hessah Hamed Alkhattabi\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png\">\n"},{"metadata":{},"cell_type":"markdown","source":"## Problem Statment"},{"metadata":{},"cell_type":"markdown","source":"As a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. However, we can confidently say that more attributes influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, we built a machine learning model that predicts the final price of each home."},{"metadata":{},"cell_type":"markdown","source":"## Executive Summary"},{"metadata":{},"cell_type":"markdown","source":"As a second project in our Data Science Immersive Course with General Assembly and MiSK Academy, we were asked to finish this \"House Prices\" Competition in Kaggle, We used multiple data cleaning methods, employed EDA methods including a good number of visualizations, to get to know the data well. Finally, we applied multiple machine learning methods in order to predict the Sale Price of the houses in the test data set. We achieved an amazing score that we are very proud of. \n \n \n Root-Mean-Squared-Error (RMSE)  = 0.11890"},{"metadata":{},"cell_type":"markdown","source":"### Contents:\n- [Datasets Description](#Datasets-Description)\n- [Data Import & Cleaning](#Data-Import-and-Cleaning)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n- [Data Visualization](#Visualize-the-data)\n- [Descriptive and Inferential Statistics](#Descriptive-and-Inferential-Statistics)\n- [Outside Research](#Outside-Research)\n- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"},{"metadata":{},"cell_type":"markdown","source":"## Datasets Description"},{"metadata":{},"cell_type":"markdown","source":"#### We were provided with four datasets to complete this challange:\n- train.csv - the training set ( with 81 columns, and 1460 rows)\n- test.csv - the test set (with 80 columns, and 1460 rows)\n- data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n- sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms"},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning and Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all the used libraries in this project\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom scipy import stats\n\nfrom sklearn import datasets, metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn import decomposition, datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nfrom math import sqrt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import neighbors\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom xgboost import XGBRegressor # just for fun :) \nfrom scipy.special import boxcox, inv_boxcox\n \nplt.style.use('ggplot')\nsns.set(font_scale = 1.5)\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Pallets used for visualizations\ncolor= \"Spectral\"\ncolor_plt = ListedColormap(sns.color_palette(color).as_hex())\ncolor_hist = 'teal'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Read CSV file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Both train and test files\ndf_full = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_full\n\ntest_df['SalePrice'] = 0\n# saving the IDs for the first and last data point in the test set \n# because we will be merging both train and test\ntest_first_id = test_df['Id'].iloc[0]\ntest_last_id = test_df['Id'].iloc[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Display data\n\nPrint the first 10 rows of each dataframe to your jupyter notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining both train and test datasets\ndf = df.append(test_df, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making sure that test set was appended to the main df\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Briefly describe the data\n\nNote things about what the columns might mean, and the general information that is conveyed in the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4a. How complete is the data? and any Issues"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()[df.isnull().sum() > 0].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding missing data and the percentage of it in each column\ntotal = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum() / df.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis = 1, keys = ['Total_NaN', 'Percent_Nan'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize the missing data \nplt.figure(figsize = (19, 10))\nsns.heatmap(data = df.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. What are your data types? "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6. fill null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill missing values with NA in Categorical Columns\ncat_bsmt_col = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ncat_multi_col = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\ncat_Garage_col = ['GarageType', 'GarageCond', 'GarageFinish', 'GarageQual']\n\ndf[cat_bsmt_col] = df[cat_bsmt_col].fillna('No_Basement')\ndf[cat_multi_col] = df[cat_multi_col].fillna('No')\ndf[cat_Garage_col] = df[cat_Garage_col].fillna('No_Garage')\ndf['MasVnrType']= df['MasVnrType'].fillna('No_MasVnr')\n\n# numerical values\ndf['Electrical'].fillna(df['Electrical'].mode().iloc[0], inplace = True)\ndf['LotFrontage'].fillna(df['LotFrontage'].median(), inplace = True) #right skewed\ndf['GarageYrBlt'].fillna(df['YearBuilt'], inplace = True) #left skewed\ndf['MasVnrArea'].fillna(df['MasVnrArea'].median(), inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if all columns are  filled \ndf.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These are the null in test\ndf.isna().sum()[df.isnull().sum() > 0].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill missing data with mode as there are very few missing data\n# we do not need to use other complex methods for filling data\ndf.fillna(df.mode().iloc[0], inplace = True)\ndf.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# head of categorical columns\ndf[df.select_dtypes('object').columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# head of numerical columns\ndf[df.select_dtypes('number').columns].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting Categorical values that are considered rankings to numerical "},{"metadata":{},"cell_type":"markdown","source":"### workflow steps:\n    1- Convert all columns that are rankings to numbers\n    2- check models preformace (got worst)\n    3- Find corretaion to Sale Prices\n    4- Recording corr values for reference\n    5- Proceed with converting for only columns that have high corr with target.\n    4- check models preformace (improved)\n    \n### Note:   \nThese steps were followed after building all the models and testing them, their scores were not optimal, therefore, we thought we can improve the results by converting ordinal categorical columns that imply ranking to numerical values."},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################### Mapping all quality columns to ranking numbers\nranking_columns = ['ExterQual', 'BsmtQual', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond']\n# Removed: 'PoolQC', 'ExterCond', 'BsmtCond',\n\nqual_dict = {\"NA\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n\nfor column in ranking_columns:\n    col = np.array(df[column].map(qual_dict), np.int16)\n    df[column] = col\n    \n#Corr with Saleprice: \n#ExterQual    =     0.686756\n#KitchenQual  =     0.662236\n#BsmtQual     =     0.586674\n#FireplaceQu  =     0.521144\n#HeatingQC    =     0.428024\n#GarageQual   =     0.273898\n#GarageCond   =     0.263249\n#BsmtCond     =     0.212632\n#ExterCond    =     0.018865\n#PoolQC       =     0.124084\n\n####################################################### Mapping basement quality columns to ranking numbers  \n#BsmtExposure    = 0.376309\n#'BsmtFinType1'  = 0.305372\n#'BsmtFinType2'  = -0.011422\n\nbasement_columns = ['BsmtFinType1']\nbasement_dict = {'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6 }    \n#Removed: , 'BsmtFinType2'\nfor column in basement_columns:\n    col = np.array(df[column].map(basement_dict), np.int16)\n    df[column] = col  \n    \n# BsmtExposure   =   0.376309 \nBsmtExposure_col = np.array(df['BsmtExposure'].map({'NA':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4}), np.int16)\ndf['BsmtExposure'] = BsmtExposure_col\n\n####################################################### Mapping garage quality columns to ranking numbers  \n# GarageFinish  = 0.550255\nGarageFinish_col = np.array(df['GarageFinish'].map({'NA':0, 'Unf':1, 'RFn':2, 'Fin':3 }), np.int16)\ndf['GarageFinish'] = GarageFinish_col    \n\n#CentralAir =    0.251328\ndf['CentralAir'] = df['CentralAir'].map({'N':0, 'Y':1}).astype(int) \n\n#PavedDrive =    0.233281\ndf['PavedDrive'] = df['PavedDrive'].map({'N':0, 'P':1, 'Y':3}).astype(int) \n\n#LandSlope =  -0.051779\ndf['LandSlope'] = df['LandSlope'].map({'Sev':0, 'Mod':1, 'Gtl':3}).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just to make sure again no Null values after conversion \ndf.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7- Data Visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"# new dataframe for only train set to visualize relationships and corelations \nvisual_df = df.iloc[0:(df[df['Id'] == test_first_id].index[0]), :] # train set\nvisual_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding correlation of all numerical columns with target\nvisual_df.corr()['SalePrice'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just to check if any of the categorical columns has a high correlation with SalePrice\ncat_columns = visual_df[visual_df.select_dtypes('object').columns]\nfor column in cat_columns:\n    cat_columns[column] = visual_df[column].astype('category').cat.codes\n    \ncat_columns['SalePrice'] = visual_df['SalePrice']\ncat_columns.corr()['SalePrice'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Normality for SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots( figsize=(15, 6))\nax.hist(visual_df['SalePrice'], bins = 300, color = color_hist)\n\nax.set_xlabel('SalePrice')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution of Sale Price Before Transformation', fontsize = 20)\n\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph above it shows that the sales price lies between 100k and 250k. Also, it shows alot of outlires on the right side."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (14, 6))\nres = stats.probplot(visual_df['SalePrice'], plot = plt)\nfig.suptitle('Probability Plot of Sale Price Before Transformation', fontsize = 20)\n\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying log transform on the data to making it normally distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots( figsize = (15, 6))\nax.hist(np.log(visual_df['SalePrice']), bins = 300, color = color_hist)\n\nax.set_xlabel('SalePrice')\nax.set_ylabel('Frequency')\n\nfig.suptitle('The Distribution of Sale Price After  log Transformation', fontsize = 20)\n\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (14, 6))\nres = stats.probplot(np.log1p(visual_df['SalePrice']), plot = plt)\n\nfig.suptitle('Probability Plot of Sale Price After log Transformation', fontsize = 20)\n\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Why are we thinking about transforming the 'SalePrice' with log**\n\nWe can see that the distribution of the target was right-skewed, and we cannot really drop all the outliers that are affecting the distribution. However, after the log, the target became normally distributed, and much better and it will be easier to fit models and to predict correct values. Therefore, we will change the target to log now, and after prediction, we will use exponentiation.\n\nIn a very useful article about log in Medium:\n> It is useful if and only if the distribution of the target variable is right-skewed, which can be observed by a simple histogram plot. This occurs when there are outliers that can’t be filtered out as they are important to the model.\n\nsourse: (https://medium.com/towards-artificial-intelligence/when-and-why-to-use-log-transformation-in-regression-6a326d6259e6)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying log to target\ndf['SalePrice'] = np.log(df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Detecting for outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the columns with the highest corelation with salePrice, and clean them from outliers\nhigh_corr = visual_df.corr()['SalePrice'].sort_values(ascending = False).head(10)\nhigh_corr.index.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# overview of all plots\nsns.pairplot(visual_df[high_corr.index.to_list()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Columns with high Correlation with the target individually to visualize distribution and outliers "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots( figsize = (12, 8))\nax = sns.scatterplot(x = 'ExterQual', \n                     y = 'SalePrice', \n                     data = visual_df, \n                     marker = 'o', s = 200, palette = color)\n\nax.set_ylabel('Sale Price')\nax.set_xlabel('The quality of the material on the exterior')\nfig.suptitle('The Guality of the Material on the Exterior vs. Sales Price', fontsize = 20)\n\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution is ok, maybe two potential outliers when the price is higher than 700,000"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots( figsize = (12, 8))\nax = sns.scatterplot(x = 'OverallQual', \n                     y = 'SalePrice', \n                     data = visual_df, \n                     marker = 'o', s = 200, palette = color)\n\nax.set_ylabel('Sale Price')\nax.set_xlabel('Overall Quality')\nfig.suptitle('Overall Quality vs. Sales Price', fontsize = 20)\n\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution is ok, maybe two potential outliers when the price is higher than 700,000, same as above."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots( figsize = (12, 8))\nax = sns.scatterplot(x = 'GrLivArea', \n                     y = 'SalePrice', \n                     data = visual_df, \n                     marker = 'o', s = 200, palette = color)\n\nax.set_ylabel('Sale Price')\nax.set_xlabel('Ground living area')\nfig.suptitle('Ground living Area vs. Sales Price', fontsize = 20)\n\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a positive correlation between ground living area and sale price, but for sure there are two outliers when the price is lower than 200,000,and ground leving area is higher than 4000. We need to drop these two outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots( figsize = (12, 8))\nax = sns.scatterplot(x = 'TotalBsmtSF', \n                     y = 'SalePrice', \n                     data = visual_df, \n                     marker = 'o', s = 200, palette = color)\n\nax.set_ylabel('Sale Price')\nax.set_xlabel('TotalBsmtSF')\nfig.suptitle('Total square feet of basement area vs. Sales Price', fontsize = 20)\n\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No need to check outliers in TotalBsmtSF because the multiconriality with 1stFlrSF, so we dropped it. There is one outlier which is TotalBsmtSF is higher than 6000.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots( figsize = (12, 8))\nax = sns.scatterplot(x = '1stFlrSF', \n                     y = 'SalePrice', \n                     data = visual_df, \n                     marker = 'o', s = 200, palette = color)\n\nax.set_ylabel('Sale Price')\nax.set_xlabel('1stFlrSF')\nfig.suptitle('First Floor Square Feet vs. Sales Price', fontsize = 20)\n\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is one outlier that we will drop, which is when 1stFlrSF is higher than 4000"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_visual_df = visual_df[visual_df.select_dtypes('object').columns]\ncat_visual_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8. Create a data dictionary"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"|Feature|Type|Dataset|Description|\n|---|---|---|---|\n|Id|int|df|Id of the house|\n|MSSubClass|int|df|Identifies the type of dwelling involved in the sale|\n|MSZoning|object|df|Identifies the general zoning classification of the sale|\n|LotFrontage|float|df|Linear feet of street connected to property|\n|LotArea|int|df|Lot size in square feet|\n|Street|object|df|Type of road access to property| \n|Alley|object|df|Type of alley access to property| \n|LotShape|object|df|General shape of property|\n|LandContour|object|df|Flatness of the property|\n|Utilities|object|df|Type of utilities available|\n|LotConfig|object|df|Lot configuration|\n|LandSlope|object|df|Slope of property|\n|Neighborhood|object|df|Physical locations within Ames city limits|\n|Condition1|object|df|Proximity to various conditions|\n|Condition2|object|df|Proximity to various conditions (if more than one is present)|\n|BldgType|object|df|Type of dwelling|\n|HouseStyle|object|df|Style of dwelling|\n|OverallQual|int|df|Rates the overall material and finish of the house|\n|OverallCond|int|df|Rates the overall condition of the house|\n|YearBuilt|int|df|Original construction date|\n|YearRemodAdd|int|df|Remodel date (same as construction date if no remodeling or additions)|\n|RoofStyle|object|df|Type of roof|\n|RoofMatl|object|df|Roof material|\n|Exterior1st|object|df|Exterior covering on house|\n|Exterior2nd|object|df|Exterior covering on house (if more than one material)| \n|MasVnrType|object|df|Masonry veneer type|\n|MasVnrArea|float|df|Masonry veneer area in square feet|\n|ExterQual|object|df|Evaluates the quality of the material on the exterior|\n|ExterCond|object|df|Evaluates the present condition of the material on the exterior|\n|Foundation|object|df|Type of foundation|\n|BsmtQual|object|df|Evaluates the height of the basement|\n|BsmtCond|object|df|Evaluates the general condition of the basement|\n|BsmtExposure|object|df|Refers to walkout or garden level walls|\n|BsmtFinType1|object|df|Rating of basement finished area|\n|BsmtFinSF1|int|df|Type 1 finished square feet|\n|BsmtFinType2|object|df|Rating of basement finished area (if multiple types)|\n|BsmtFinSF2|int|df|Type 2 finished square feet|\n|BsmtUnfSF|int|df|Unfinished square feet of basement area|\n|TotalBsmtSF|int|df|Total square feet of basement area|\n|Heating|object|df|Type of heating|\n|HeatingQC|object|df|Heating quality and condition|\n|CentralAir|object|df|Central air conditioning|\n|Electrical|object|df|Electrical system|\n|1stFlrSF|int|df|First Floor square feet|\n|2ndFlrSF|int|df|Second floor square feet|\n|LowQualFinSF|int|df|Low quality finished square feet (all floors)|\n|GrLivArea|int|df|Above grade (ground) living area square feet|\n|BsmtFullBath|int|df|Basement full bathrooms|\n|BsmtHalfBath|int|df|Basement half bathrooms|\n|FullBath|int|df|Full bathrooms above grade|\n|HalfBath|int|df|Half baths above grade|\n|BedroomAbvGr|int|df|Bedrooms above grade (does NOT include basement bedrooms)|\n|KitchenAbvGr|int|df|Kitchens above grade|\n|KitchenQual|object|df|Kitchen quality|\n|TotRmsAbvGrd|int|df|Total rooms above grade (does not include bathrooms)|\n|Functional|object|df|Home functionality (Assume typical unless deductions are warranted)|\n|Fireplaces|int|df|Number of fireplaces|\n|FireplaceQu|object|df|Fireplace quality|\n|GarageType|object|df|Garage location|\n|GarageYrBlt|float|df|Year garage was built|\n|GarageFinish|object|df|Interior finish of the garage|\n|GarageCars|int|df|Size of garage in car capacity|\n|GarageArea|int|df|Size of garage in square feet|\n|GarageQual|object|df|Garage quality|\n|GarageCond|object|df|Garage condition|\n|PavedDrive|object|df|Paved driveway|\n|WoodDeckSF|int|df|Wood deck area in square feet|\n|OpenPorchSF|int|df|Open porch area in square feet|\n|EnclosedPorch|int|df|Enclosed porch area in square feet|\n|3SsnPorch|int|df|Three season porch area in square feet|\n|ScreenPorch|int|df|Screen porch area in square feet|\n|PoolArea|int|df|Pool area in square feet|\n|PoolQC|object|df|Pool quality|\n|Fence|object|df|Fence quality|\n|MiscFeature|object|df|Miscellaneous feature not covered in other categories|\n|MiscVal|int|df|$Value of miscellaneous feature|\n|MoSold|int|df|Month Sold (MM)|\n|YrSold|int|df|Year Sold (YYYY)|\n|SaleType|object|df|Type of sale|\n|SaleCondition|object|df|Condition of sale|"},{"metadata":{},"cell_type":"markdown","source":"## Finding correlation between columns and visualize it"},{"metadata":{},"cell_type":"markdown","source":"#### Use Seaborn's heatmap with pandas `.corr()` to visualize correlations between all numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find corelations between all columns and the target\ndf.corr()['SalePrice'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize = (16, 14)) \nmask = np.triu(np.ones_like(visual_df.corr(), dtype = np.bool))\ng = sns.heatmap(visual_df.corr(), ax = axs, mask=mask, cmap = sns.diverging_palette(180, 10, as_cmap = True), square = True)\n\nplt.title('Correlation between Features')\n\n# fix for mpl bug that cuts off top/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heatmap above, we can see that there are several features highly correlated, and these will cause multicollinearity. We need to drop one of them.\n\n- YearBuilt and GarageYrBlt, this is reasonable since many times YearBuilt and GarageYrBlt will be the same. Drop GarageYrBlt\n- GrLivArea and TotRmsAbvGrd, drop TotRmsAbvGrd\n- 1stFlrSF and TotalBsmtSF, drop TotalBsmtSF\n- GarageCars & GarageArea, tried to drop GarageCars, but preformance got worst. Therefor, keeping both."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['GarageYrBlt', 'TotRmsAbvGrd', 'TotalBsmtSF'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing only coulmns with high correlation with the target"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = visual_df.corr()\ntop_corr_features = corr_matrix.index[abs(corr_matrix['SalePrice']) > 0.5]\n\nfig, axs = plt.subplots(figsize = (13, 8)) \nmask = np.triu(np.ones_like(visual_df[top_corr_features].corr(), dtype = np.bool))\nsns.heatmap(visual_df[top_corr_features].corr(), ax = axs, annot = True, mask = mask, cmap = sns.diverging_palette(180, 10, as_cmap = True))\nplt.title('Correlation of high correlated columns with Sale Price')\n\n# fix for mpl bug that cuts off top/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The distribution of Numerical Columns in the Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function that takes a dataframe and transforms it into a standard form after dropping nun_numirical columns\ndef to_standard (df):\n    \n    num_df = df[df.select_dtypes(include = np.number).columns.tolist()]\n    \n    ss = StandardScaler()\n    std = ss.fit_transform(num_df)\n    \n    std_df = pd.DataFrame(std, index = num_df.index, columns = num_df.columns)\n    return std_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(1, 1, figsize = (18, 18))\nplt.title('The distribution of All Numeric Variable in the Dataframe', fontsize = 20) #Change please\n\nsns.boxplot(y = \"variable\", x = \"value\", data = pd.melt(to_standard(visual_df)), palette = color)\nplt.xlabel('Range after Standarization', size = 16)\nplt.ylabel('Attribue', size = 16)\n\n\n# fix for mpl bug that cuts off top/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(1, 1, figsize = (18, 8))\nplt.title('The distribution of All Numeric Variable in the Dataframe', fontsize = 20) #Change please\n\nsns.boxplot(y = \"variable\", x = \"value\", data = pd.melt(to_standard(visual_df[top_corr_features])), palette = color)\nplt.xlabel('Range after Standarization', size = 16)\nplt.ylabel('Attribue', size = 16)\n\n\n# fix for mpl bug that cuts off top/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking skewness of all numerical columns"},{"metadata":{},"cell_type":"markdown","source":"### Workflow Steps:\n1. Building and testing models without any transformation, good enough results, but not the best\n2. Applying the Box Cox transformation to all columns, got better results\n3. Tried to reduce the number of coulmns being transformed by only choosing the columns with high skewness (>4.00), we got even better results.\n4. As a final step, we also transformed the columns that have high correlation with the target and have have some skewness, we got the best results. \n\n### Note\nWe actually applied this transformation method after building and testing the model, the RMSE score improved drastically after the Box Cox transformation of some columns.\n> A Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn’t normal, applying a Box-Cox means that you are able to run a broader number of tests.\n\nSourse: (https://www.statisticshowto.datasciencecentral.com/box-cox-transformation/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = visual_df.dtypes[visual_df.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = visual_df[numeric_feats.tolist()].apply(lambda x:stats.skew(x.dropna())).sort_values(ascending = False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew':skewed_feats})\nskewness.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skewed_features =['MiscVal', 'PoolArea', 'LotArea', '3SsnPorch', 'LowQualFinSF', \n                  'KitchenAbvGr','BsmtFinSF2', 'ScreenPorch', 'GrLivArea', 'ExterQual',\n                  'BsmtHalfBath']\n\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\n\n#skewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    df[feat] = boxcox(df[feat], lam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting dummies for all categorical columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing all Categorical columns to dummies (0,1)\ndf = pd.get_dummies(df, columns = df.select_dtypes('object').columns, drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data from from test_first_id to the end to be in the test\ntest_df = df.iloc[df[df['Id'] == test_first_id].index[0]:, :]\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# deleting the test dataset from the main df\ndf = df.iloc[0:(df[df['Id'] == test_first_id].index[0]), :]\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outlier removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing GrLivArea outliers, also dropping SalePrice > 700,000 by default, which is good.\nprint(df.shape)\ndf.drop(df.index[[523, 1298]], inplace = True)\ndf = df.drop(df[(df['GrLivArea'] > 4000) & (df['SalePrice'] < 300000)].index)\ndf = df.drop(df[df['1stFlrSF'] >= 3000].index)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No need for the ID column\ndf = df.drop('Id', axis = 1)\n\ntest_df = test_df.drop(['Id','SalePrice'] , axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kaggle Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function that gets the predictions and saves them into a csv file with the correct format\ndef submission_file (test_pred):\n    for_id = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\n    my_submission = pd.DataFrame({'Id':for_id.Id, 'SalePrice':test_pred.reshape(1459)})\n    my_submission.to_csv('submission.csv', index = False) # dropping the index column before saving it","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying Machine learning models for predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"BOLD = '\\033[1m'\nEND = '\\033[0m'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['SalePrice']\nX = df.drop('SalePrice', axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .10, shuffle = True, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function that gets all datasets and model, and will fit and calculates all metrics, and return predictions\ndef model_metrics(model, kfold, X_train, X_test, y_train, y_test, test_df):\n\n    model.fit(X_train, y_train)\n\n    #metrics -> R squared\n    results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = 'r2')\n    print(\"CV scores: \", results); print(\"CV Standard Deviation: \", results.std()); print();\n    print('CV Mean score: ', results.mean()); \n    print('Train score:   ', model.score(X_train, y_train))\n    print('Test score:    ', model.score(X_test, y_test))\n      \n    MSE = -(cross_val_score(model, X_train, y_train, cv = kfold, scoring = 'neg_mean_squared_error').mean())\n    print(\"CV MSE:        \",MSE)\n    \n    RMSE = sqrt(MSE)\n    print(\"CV RMSE:       \",RMSE)\n    \n    test_pred = model.predict(test_df)\n    test_pred_exp = np.exp(test_pred)\n    \n    return test_pred_exp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multiple models"},{"metadata":{},"cell_type":"markdown","source":"Multiple initial models to check base line"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Multi_models (X_train, X_test, y_train, y_test, test_df):\n    kfold = 5\n#     # Create an scaler object\n#     ss = StandardScaler()\n#     X_train = ss.fit_transform(X_train)\n#     X_test = ss.transform(X_test)\n#     test_df = ss.transform(test_df)\n#     y_train = ss.transform(y_train)\n#     y_test = ss.transform(y_test)\n###################################################################################################### Linear Regression model\n    print(BOLD + 'Linear Regression model:' + END)\n    \n    lr = LinearRegression()\n    lr_pred= model_metrics(lr, kfold, X_train, X_test, y_train, y_test, test_df)\n    \n######################################################################################################  Lasso model\n    print(); print(BOLD + 'Lasso model:' + END)\n    \n    alpha = np.arange(0, 3, 200)\n    lasso = Lasso(alpha = alpha, max_iter = 50000)\n    lasso_pred = model_metrics(lasso, kfold, X_train, X_test, y_train, y_test, test_df)\n\n######################################################################################################  Ridge model\n    print(); print(BOLD + 'Ridge model:' + END)\n    \n    ridge_alpha_values = np.logspace(0, 5, 200)\n    ridgecv_optimal = RidgeCV(alphas = ridge_alpha_values, cv = 10)\n    ridge_pred = model_metrics(ridgecv_optimal, kfold, X_train, X_test, y_train, y_test, test_df)\n    \n######################################################################################################  Elastic Net model\n    print(); print(BOLD + 'Elastic Net model:' + END)\n    \n    elasticnet = ElasticNet(alpha = 0.01)\n    elasticnet_pred = model_metrics(elasticnet, kfold, X_train, X_test, y_train, y_test, test_df)\n    \n######################################################################################################  Decision Tree Regressor model\n    print(); print(BOLD + 'Decision Tree Regressor model:' + END)\n    \n    dtr = DecisionTreeRegressor()\n    dtr_pred = model_metrics(dtr, kfold, X_train, X_test, y_train, y_test, test_df)\n    \n######################################################################################################  K Neighbors Regressor model\n    print(); print(BOLD + 'K Neighbors Regressor model:' + END)\n    \n    KNN = neighbors.KNeighborsRegressor()\n    KNN_pred = model_metrics(KNN, kfold, X_train, X_test, y_train, y_test, test_df)\n    \n######################################################################################################  Random Forest Regressor model   \n    print(); print(BOLD + 'Random Forest Regressor model:' + END)\n\n    rfr = RandomForestRegressor(n_estimators = 100, oob_score = True, random_state = 42)\n    rfr_pred = model_metrics(rfr, kfold, X_train, X_test, y_train, y_test, test_df)\n    \n    #submission_file (dtr_pred)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Multi_models (X_train, X_test, y_train, y_test, test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Model"},{"metadata":{},"cell_type":"markdown","source":"The main Machine learning model that gave us the best score, which is Ridge model using RidgeCV to get the optimal alpha. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def ridge__optimizer(X_train, X_test, y_train, y_test, test_df):\n    print(); print(BOLD + 'Ridge model (best so far with 0.12100 kaggle score):' + END)\n    kfold = 5\n    \n    \n    ridge_alpha_values = np.logspace(0, 5, 200)\n\n    ridgecv_optimal = RidgeCV(alphas = ridge_alpha_values, cv = 10)\n    ridgecv_optimal.fit(X_train, y_train)\n\n    print('Optimal Alpha:   ' , ridgecv_optimal.alpha_)\n    \n    # Create a logistic regression object with an L2 penalty\n    ridge = Ridge(alpha = ridgecv_optimal.alpha_)\n\n    \n    ridge_opt_pred = model_metrics(ridge, kfold, X_train, X_test, y_train, y_test, test_df)\n    print(ridge_opt_pred)\n    \n    #submission_file (ridge_opt_pred) \n    #The line for saving the predictions for submission gives an error in Kaggle, therefor we commented it here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge__optimizer(X_train, X_test, y_train, y_test, test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Trying to Find the best Alpha for ridge**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = []\n# check the below alpha values for Ridge Regression\nalpha = np.arange(0.0001, 10, 200)\n\nfor alph in alpha:\n    ridge = Ridge(alpha = alph, copy_X = True, fit_intercept = True)\n    ridge.fit(X_train, y_train)\n    predict = ridge.predict(X)\n    rmse.append(np.sqrt(mean_squared_error(predict, y)))\nprint(rmse)\nplt.scatter(alpha, rmse)\nrmse = pd.Series(rmse, index = alpha)\nprint(rmse.argmin())\nprint(rmse.min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# trying KNN regression model with a stander scaler and grid search for multiple k values\ndef KNN_opt_model (X_train, X_test, y_train, y_test, test_df):\n    kfold = 5\n    \n    print(); print(BOLD + 'K Neighbors Regressor model:' + END)\n\n    # Create an scaler object\n    ss = StandardScaler()\n\n    # Create a logistic regression object with an L2 penalty\n    KNN = neighbors.KNeighborsRegressor()\n\n    # Create a pipeline of three steps. First, standardize the data.\n    # Second, tranform the data with PCA.\n    # Third, train a Decision Tree Classifier on the data.\n    pipe = Pipeline(steps = [('ss', ss),\n                           ('KNN', KNN)])\n    \n    # Create lists of parameter for KNeighborsRegressor()\n    n_neighbors = [5, 10, 15]\n    algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n\n    # Create a dictionary of all the parameter options \n    # Note has you can access the parameters of steps of a pipeline by using '__’\n    parameters = dict(KNN__n_neighbors = n_neighbors,\n                      KNN__algorithm = algorithm)\n\n    # Conduct Parameter Optmization With Pipeline\n    # Create a grid search object\n    clf = GridSearchCV(pipe, parameters)\n    \n    KNN_pred = model_metrics(KNN, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (KNN_pred)\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN_opt_model(X_train, X_test, y_train, y_test, test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lasso_optimizer(X_train, X_test, y_train, y_test, test_df, X, y):\n    print(); print(BOLD + 'Optimized Lasso model:' + END)\n    kfold = 5\n    optimal_lasso = LassoCV(n_alphas = 500, cv = 10, verbose = 1)\n    optimal_lasso.fit(X_train, y_train)\n    print('optimal_lasso:    ', optimal_lasso.alpha_)\n    \n    # Create a logistic regression object with an L2 penalty\n    lasso = Lasso(alpha = optimal_lasso.alpha_)\n\n    lasso_pred = model_metrics(lasso, kfold,  X_train, X_test, y_train, y_test, test_df)\n    submission_file(lasso_pred)\n    \n    lasso.fit(X, y)\n\n    lasso_coefs = pd.DataFrame()\n    lasso_coefs['Column_name'] = X.columns\n    lasso_coefs['coefficient'] = lasso.coef_\n    lasso_coefs['absolute_coefficient'] = np.abs(lasso.coef_)\n\n    lasso_coefs = lasso_coefs.sort_values('absolute_coefficient', ascending = False)\n    print('Percent variables zeroed out:', np.sum(lasso.coef_ == 0) / X.iloc[:, 0].count())\n\n    lasso_coefs.head(15)\n    return lasso_coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_coefs = lasso_optimizer(X_train, X_test, y_train, y_test, test_df, X, y);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_coefs.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Elastic Net Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ElasticNet__optimizer(X_train, X_test, y_train, y_test, test_df, X, y):\n    \n    kfold = 5\n    \n    l1_ratios = np.linspace(0.01, 1.0, 25)\n\n    optimal_enet = ElasticNetCV(l1_ratio = l1_ratios, n_alphas = 100, cv = 10, verbose = 1)\n    optimal_enet.fit(X, y)\n\n    print(); print(BOLD + 'Elastic Net model:' + END)\n    print('Optimal alpha:       ', optimal_enet.alpha_)\n    print('Optimal l1 ratio  :  ', optimal_enet.l1_ratio_)\n    \n    enet = ElasticNet(alpha = optimal_enet.alpha_, l1_ratio = optimal_enet.l1_ratio_)\n    \n\n    enet_pred = model_metrics(enet, kfold,  X_train, X_test, y_train, y_test, test_df)\n    #submission_file (enet_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ElasticNet__optimizer(X_train, X_test, y_train, y_test, test_df, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Regressor Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying DecisionTreeRegressor Model \n\nprint(BOLD + 'Decision Tree Regressor model:' + END)\n\n\ndecision_tree = DecisionTreeRegressor( max_depth = 10, random_state = 33)\ndecision_tree.fit(X_train, y_train)\n\n#Calculating Training & Testing Scores\nprint('Train Score: ', decision_tree.score(X_train, y_train))\nprint('Test Score is : ', decision_tree.score(X_test, y_test))\nprint('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = decision_tree.predict(X_test)\n\n#----------------------------------------------------\n#Calculating MAE\nMAE_value = mean_absolute_error(y_test, y_pred, multioutput = 'uniform_average') \nprint('MAE Score: ', MAE_value)\n\n#----------------------------------------------------\n#Calculating MSE\nMSE_value = mean_squared_error(y_test, y_pred, multioutput = 'uniform_average') \nprint('MSE Score: ', MSE_value)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation and Conceptual Understanding"},{"metadata":{},"cell_type":"markdown","source":"After evaluating all of the applied models, we can confidently say that the best model without any competitors was Ridge with the optimal alpha value. The rest of the Lasso and ElasticNet performed slightly worst that ridge, even after finding the optimal alpha. Other models like KNN and leaner regression did not perform well. Decision tree and random forest had overfitting as the train scores were too hight compared to the test. The below scores were for the best preforming model (Ridge) that got us the RMSE of  0.11890"},{"metadata":{},"cell_type":"markdown","source":"* Ridge model (best so far with 0.12100 kaggle score):\n* Optimal Alpha:    12.216773489967919\n* CV scores:  [0.89327908 0.89926145 0.90971806 0.93280416 0.9311133 ]\n* CV Standard Deviation:  0.016176855310963398\n* CV Mean score:  0.913235211102004\n* Train score:    0.9375894112878308\n* Test score:     0.9321056236353693\n* CV MSE:         0.013403207809351957\n* CV RMSE:        0.11577222382485342\n* [117563.4133125  155207.48419969 181581.10791461 ... 173173.10705228\n 115697.95594533 223288.03456509]"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"\nAs a second project in our Data Science Immersive Course with General Assembly and MiSK Academy, we were asked to finish this \"House Prices\" Competition in Kaggle, We used multiple data cleaning methods, employed (EDA) methods including a good number of visualizations, to get to know the data well, and we preprocessed our dataset with some transformation methods. Finally, we applied multiple machine learning methods in order to predict the Sale Price of the houses in the test data set. The regrission models used were Linear, Ridge, Lasso, Elastic Net, KNN, Decision Tree, Random Forest and a few more models. We achieved an amazing score in Kaggle competition that we are very proud of, we were ranked as the 607th out of 4,675 teams. \n \n Root-Mean-Squared-Error (RMSE)  = 0.11890\n \nThank you very mcuh,\nRaghad, Fatmah, Hessah\n "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}